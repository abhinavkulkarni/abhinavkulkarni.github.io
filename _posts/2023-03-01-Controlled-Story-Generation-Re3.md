---
layout: post
title:  "Generative AI: Controlled Story Generation ($\\text{Re}^3$)"
excerpt: In this blog post, we discuss a paper from Facebook AI Research (and other associates) about how to automatically generate longer stories with a given outline in a controlled manner while maintaining the story's coherence and relevance.
date:   2023-03-01 01:21:13 -0800
comments: true
categories: jekyll update
---

In this blog post, I'll try to summarize a controlled story generation method from Facebook AI Research - [$\text{Re}^3$](re3-link) (Recursive Reprompting and Revision framework). It is [open source](github-link) and has Python libraries available. $\text{Re}^3$ can generate stories of over two thousand words. Some of the challenges in automatically generating longer stories are as follows:

1. **Limited Context**: Most large language models (LLMs) such as [InstructGPT][instructgpt-link] have a limited context of a few thousand tokens ([link](gpt3-link)). Thus, fitting the entire story into a single prompt is not possible.
2. **Coherence**: The generated story should be coherent (i.e. it should _"make sense"_).
3. **Relevance**: The generated story should be relevant to the given outline.
4. **Factuality**: The generated story should be factually correct (that is it should get details such as the genders of the characters, and their relationships correct throughout the story).

## $\text{Re}^3$ Framework

At a high level, $\text{Re}^3$ has following modules that aim to mimic the process of human writing:

1. **Planning**: Plan module generates a plan by prompting GPT3 to augment a given premise with a setting, characters, and outline.

2. **Drafting**: Given a story generated so far, Draft module generates multiple possible next story continuations by recursively reprompting GPT3.

3. **Rewriting**: Rewrite module selects the most relevant and coherent story continuation from the Draft module.

4. **Editing**: Edit module makes smaller local edits to improve factual consistency with previous passages

<p align="center">
  <img src="/assets/re3/re3-overview.png" />
</p>

We will now discuss each of these modules in detail.

### 1. Plan Module

The plan module contains the following data points:

1. Premise
2. Setting
3. Characters
4. Outline

Of these, the premise is entered by the user, the module fills the rest.

E.g., for an input premise:

**PREMISE**

> A new law grad returns home to start her career, but struggles with the broken justice system.


The plan module generates the following:

**SETTING**:

>  The story is set in <span style="color:green">*a small town in the United States.*</span>

**CHARACTERS**:

>  1. Character Portrait: <span style="color:green">*Liza Turner is a 22-year-old woman.*</span>
>  2. Character Portrait: <span style="color:green">*Peyton Turner is Liza’s older sister.*</span>

**OUTLINE**:

> Outline the main plot points of the story.
> 1. <span style="color:green">*Liza Turner graduates from law
school.*
> 2. <span style="color:green">*She moves back to her hometown to
start her career.*
> 3. <span style="color:green">*She struggles with the reality of
the broken justice system.*

where the green highlighted texts are completed by the GPT3 model, given the corresponding prompt.

<p align="center">
  <img src="/assets/re3/plan-overview.png" />
</p>

### 2. Draft Module

For each point of the outline, the draft module generates several story passages before moving on to the next outline point. Each passage is generated as a fixed-length continuation from a prompt with a specific structure as shown in the below image.

<p align="center">
  <img src="/assets/re3/draft-overview.png" />
</p>

As the story progresses, the system dynamically updates the list of character descriptions using a named entity-recognition-based pipeline, which identifies new entities from each new story passage using [Flair](flair-link) and writes descriptions using GPT3.

1. "Relevant Context" initially contains all of the premise, setting, and characters as shown in Figure 2, but subsequently selects only what is most relevant to the most recent story passage using a pre-trained [Dense Passage Retrieval (DPR)](dpr-link) model.

2. "Previous Sections’ Outlines" has a very high-level summary of previous larger story sections.

3. "Recent Story Summary" is a summary of a few recent story passages generated by GPT3.

4. "Current Section Outline" is the current outline point.

5. "Autoregressive Context" is the previous story passage generated by GPT3.


### 3. Rewrite Module

The draft module generates multiple continuations for each outline point. The rewrite module then reranks these continuations based on their coherence and relevance. The rewrite module has two components, both of which are trained on the [WritingPrompts dataset](writingprompts-link). It is a large dataset of 300K human-written stories paired with writing prompts from a Reddit subreddit.

<p align="center">
  <img src="/assets/re3/rewrite-overview.png" />
</p>

1. **Coherence Reranker**: Authors take passages (up to 1000 tokens) from this dataset, and split them into start and end sections (the end section is up to 200 tokens long, remaining is the start section). Using contrastive learning, they train a classifier that predicts a coherence score for a given passage and proposed continuation.
<br>
<br>
2. **Relevance Reranker**: Authors take 2000 passages from the dataset, generate short summaries for each of them using GPT3 and then use contrastive learning to train a classifier that predicts, for a given passage and proposed continuation, whether the continuation is relevant to the passage.

### 4. Edit Module

The Edit module makes local edits to further refine a passage Specifically, it removes long-range factual inconsistencies. It has two steps: _detecting_ factual inconsistencies, and _correcting_ them. An inconsistency involves two statements. As the number of statement pairs scales quadratically with story length, naive approaches are infeasible. To make the task more tractable, the authors focus on factual inconsistencies in character attributes (e.g., age, occupation, and relationship to another character).

<p align="center">
  <img src="/assets/re3/edit-overview.png" />
</p>

The system maintains several attributes for each character in a knowledge base (KB). Attributes are key-value pairs such as `(gender, "female"), (workplace, "restaurant")`. The authors take inspiration from [OpenIE system](openie-link) to capture a wide variety of such attributes.

* **Detecting factual inconsistencies**: When a coherent and outline relevant continuation is generated by the rewrite module, for every character several facts are inferred by few-shot prompting GPT3. The inferred facts are then converted to key-value pair attributes and are cross-checked with the KB. If a contradiction is found, appropriate edit instructions are added to the GPT3 prompt.
<br>
<br>
Below, for a generated continuation by the rewrite module, the system infers the facts highlighted in green. A generated continuation is fed to GPT3 thrice (perhaps for different settings of the temperature parameter). If an inferred fact is present in at least two of the three prompt completions or if it is okayed by an entailment classifier, then it is retained. Authors use a [BART-large model trained on MNLI](bart-large-link) to perform entailment classification.
<br>
<br>
These inferred facts are then converted to key-value pair attributes by few-shot prompting GPT3.
<br>
<br>
Similarly, edit instructions are also generated by few-shot prompting GPT3 for attributes that are in contradiction with those in the KB.

> "I don’t know," answered Maria, "but I’ll go with you if you’re going to meet someone." so they left together. They walked down to the main part of the community where everyone else was gathering and found an older woman there named Lucy that Karen knew. "You know what’s going on don’t you?" Karen asked Lucy. "Yes, I do but I don’t think I can tell you much because I’ve been sworn to secrecy," she replied. "I just have a feeling that if we’re going to get any answers from this person we’re looking for we’re going to have to see them face-to-face," assured Lucy. They all agreed that this was the best way to do things and went looking for their neighbor Lizzy who had a car that worked. They met her again in the cafeteria and told her about their plans and Lizzy said she would like nothing better than a little adventure so she agreed to take them as long as no one got hurt along the way. 
> <br>
> <br>
> Question: List very brief facts about Lucy’s appearance, personality, and relationship to other characters. 
> 1. <span style="color:green">*Lucy is an older woman.*</span>
> 2. <span style="color:green">*Lucy is sworn to secrecy.*</span>
> 3. <span style="color:green">*Lucy is a good friend of Karen’s.*</span>

* **Correcting factual inconsistencies**: This takes a prompt with edit instructions and rewrites a corrected passage.

When a passage is finalized, new facts are inferred, key-value pair attributes are generated from those, and the KB is updated.

### Miscaellaneous

1. While $\text{Re}^3$ is capable of generating longer stories, authors experimented with generating roughly 3000 token  (2000-2500 words) long stories. Thus, they sample initial outlines until they contain exactly three points and for each outline point they generate four 256-token continuations (3 * 4 * 256 ~= 3000) before moving on to the next outline point. As a storyending mechanism, they use the [GPT3 Insert
API](gpt3-insert-api-link) to complete the story with the suffix "The End."
<br>
<br>
2. **Evaluation**: Authors track the following four metrics with the help of human judges. The results summarized in the paper.

    1. **Interesting**: Interesting to the reader.
    2. **Coherent**: Plot-coherent.
    3. **Relevant**: Faithful to the initial premise.
    4. **Humanlike**: Judged to be human-written.


[re3-link]: https://arxiv.org/abs/2210.06774
[instructgpt-link]: https://openai.com/research/instruction-following
[gpt3-link]: https://platform.openai.com/docs/models/gpt-3
[github-link]: https://github.com/yangkevin2/emnlp22-re3-story-generation
[flair-link]: https://github.com/flairNLP/flair
[dpr-link]: https://arxiv.org/abs/2004.04906
[writingprompts-link]: https://paperswithcode.com/dataset/writingprompts
[openie-link]: https://stanfordnlp.github.io/CoreNLP/openie.html
[bart-large-link]: https://huggingface.co/facebook/bart-large-mnli
[gpt3-insert-api-link]: https://openai.com/blog/gpt-3-edit-insert
